---
title: "Homework 2"
author: "Qianlang Chen"

# CS 3190
# U 09/27/20

output: pdf_document
fontsize: 12pt

header-includes:
  - \usepackage{setspace}
  - \linespread{1.25}
  - \usepackage{arydshln} # for dashed lines in matrices
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newcommand{\E} {\text{E}}
\newcommand{\Var} {\text{Var}}
\newcommand{\inpar}[1] {\left(#1\right)}

# Problem 1

## Part (a)

We *can* only use Chebyshev's Inequality to calculate the upper-bound of such probability because we know the values of $\E(X)$ and $\Var(X)$, which are all what the inequality needs.

We *cannot* use Markov's Inequality because we are not sure if $X \ge 0$ at all times, which is the requirement of using the Markov's Inequality. Even though the expect value of $X$ is 100 (quite big), there is no guarantee that $X$ never goes below zero.

We also *cannot* use the Chernoff-Hoeffding Inequality because we don't know the bounds of the variable $X$. $X$ needs to have a defined bound to use the C-H Inequality.

## Part (b)

Since $\E(X) = 100$ and $\Var(X) = 144$, by Chebyshev's Inequality,
$$
\begin{aligned}
  \Pr(X < 75) &= \Pr(X < 100 - 25)\\
              &= \Pr(X < \E(X) - 25)\\
              &= \Pr(|X - \E(X)| > 25) & (\implies \varepsilon = 25)\\
              &\le \frac{\Var(X)}{\varepsilon^2} = \frac{144}{25^2} \approx \boxed{23\%}
\end{aligned}
$$

## Part (c)

An example of that could be a normal (Gaussian) distribution. A normal distribution does not have a definite bound, which is required in order to use either Markov's or Chernoff-Hoeffding Inequality.

\pagebreak
# Problem 2

## Part (a)

First of all, by linearity of expectation, we have
$$
\begin{aligned}
  \E(\bar{X}) &= \E(\frac{1}{n} \cdot \sum_{i=1}^n X_i)\\
              &= \frac{1}{n} \cdot \sum_{i=1}^n \E(X_i)\\
              &= \frac{1}{n} \cdot n \cdot 7 = 7
\end{aligned}
$$

Moreover, since $n = 2$, we have
$$
\Var(\bar{X}) = \frac{\Var(X_i)}{n} = \frac{2}{2} = 1
$$

Now, by Chebyshev's Inequality,
$$
\begin{aligned}
  \Pr(\bar{X} > 12) &= \Pr(\bar{X} > 7 + 5)\\
                    &= \Pr(\bar{X} > \E(\bar{X}) + 5)\\
                    &= \Pr(|\bar{X} - \E(\bar{X})| > 5) & (\implies \varepsilon = 5)\\
                    &\le \frac{\Var(\bar{X})}{\varepsilon^2} = \frac{1}{5^2} = \boxed{4\%}
\end{aligned}
$$

## Part (b)

Since $\Delta = t-b = 13-1 = 12$, by the Chernoff-Hoeffding Inequality (reusing values from *Part (a)*),
$$
\begin{aligned}
  \Pr(\bar{X} > 12) &= \Pr(|\bar{X} - \E(\bar{X})| > 5) & (\implies \varepsilon = 5)\\
                    &\le 2 \cdot \exp\inpar{\frac{-2\varepsilon^2n}{\Delta^2}}\\
                    &\le 2 \cdot \exp\inpar{\frac{-2 \times 5^2 \times 2}{12^2}} \approx \boxed{99.9\%}
\end{aligned}
$$

## Part (c)

Similar to *Part (a)*, by Chebyshev's Inequality,
$$
\begin{aligned}
  \Pr(\bar{X} > 12) &= \Pr(\bar{X} > 7 + 5)\\
                    &= \Pr(\bar{X} > \E(\bar{X}) + 5)\\
                    &= \Pr(|\bar{X} - \E(\bar{X})| > 5) & (\implies \varepsilon = 5)\\
                    &\le \frac{\Var(\bar{X})}{\varepsilon^2}\\
                    &\le \frac{\Var(X_i)}{n\varepsilon^2} = \frac{2}{20 \times 5^2} = \boxed{0.4\%}
\end{aligned}
$$

## Part (d)

Similar to *Part (b)*, by the Chernoff-Hoeffding Inequality (reusing values from *Part (c)*),
$$
\begin{aligned}
  \Pr(\bar{X} > 12) &= \Pr(|\bar{X} - \E(\bar{X})| > 5) & (\implies \varepsilon = 5)\\
                    &\le 2 \cdot \exp\inpar{\frac{-2\varepsilon^2n}{\Delta^2}}\\
                    &\le 2 \cdot \exp\inpar{\frac{-2 \times 5^2 \times 20}{12^2}} \approx \boxed{0.2\%}
\end{aligned}
$$

\pagebreak
# Problem 3

## Part (a)

Let $x = -1$. Now, the vectors $\vec{p}$ and $\vec{q}$ are linearly dependent because they are scaled versions of each other:
$$
\left[\begin{array}{c}
   1\\
  -2\\
   4\\
   x
\end{array}\right] = \left[\begin{array}{c}
   1\\
  -2\\
   4\\
  -1
\end{array}\right] = \frac{1}{2} \times \left[\begin{array}{r}
   2\\
  -4\\
   8\\
   -2
\end{array}\right]
$$

## Part (b)

Let $x = 21$. Now, the vectors $\vec{p}$ and $\vec{q}$ are orthogonal because their dot-product is zero:
$$
\begin{aligned}
  \left[\begin{array}{c}
     1\\
    -2\\
     4\\
     x
  \end{array}\right] \cdot \left[\begin{array}{c}
     2\\
    -4\\
     8\\
     -2
  \end{array}\right] &= \left[\begin{array}{cccc}
     1 & -2 & 4 & x
  \end{array}\right] \left[\begin{array}{c}
     2\\
    -4\\
     8\\
     -2
  \end{array}\right]\\
                     &= \left[\begin{array}{cccc}
     1 & -2 & 4 & 21
  \end{array}\right] \left[\begin{array}{c}
     2\\
    -4\\
     8\\
     -2
  \end{array}\right]\\
                     &= 1 \times 2 + 2 \times 4 + 4 \times 8 - 21 \times 2\\
                     &= 0
\end{aligned}
$$

## Part (c)

$$
\begin{aligned}
  ||\vec{q}||_1 = \left|\left|\begin{array}{r}
     2\\
    -4\\
     8\\
     -2
  \end{array}\right|\right|_1 = |2| + |{-4}| + 8 + |{-2}| = \boxed{16}
\end{aligned}
$$

## Part (d)

$$
\begin{aligned}
  ||\vec{q}||_2^2 = \left|\left|\begin{array}{r}
     2\\
    -4\\
     8\\
     -2
  \end{array}\right|\right|_2^2 = 2^2 + (-4)^2 + 8^2 + (-2)^2 = \boxed{88}
\end{aligned}
$$

\pagebreak
# Problem 4

## Part (a)

$$
\begin{aligned}
  A^TB &= \left[\begin{array}{rrr}
    2 & -1 & 4\\
    0 & -1 & 0\\
    3 & -2 & 6
\end{array}\right]^T \left[\begin{array}{rrr}
    0 & 0 & 1\\
    1 & 0 & 0\\
    0 & 2 & 0
\end{array}\right]\\
       &= \left[\begin{array}{rrr}
     2 &  0 &  3\\
    -1 & -1 & -2\\
     4 &  0 &  6
\end{array}\right]\left[\begin{array}{rrr}
    0 & 0 & 1\\
    1 & 0 & 0\\
    0 & 2 & 0
\end{array}\right]\\
       &= \left[\begin{array}{rrr}
    2 \times 0 + 0 \times 1 + 3 \times 0 & 2 \times 0 + 0 \times 0 + 3 \times 2 & 2 \times 1 + 0 \times 0 + 3 \times 0\\
    -1 \times 0 - 1 \times 1 - 2 \times 0 & -1 \times 0 - 1 \times 0 - 2 \times 2 &  -1 \times 1 -1 \times 0 - 2 \times 0\\
    4 \times 0 + 0 \times 1 + 6 \times 0 & 4 \times 0 + 0 \times 0 + 6 \times 2 & 4 \times 1 + 0 \times 0 + 6 \times 0\\
\end{array}\right]\\
       &= \boxed{\left[\begin{array}{rrr}
     0 &  6 &  2\\
    -1 & -4 & -1\\
     0 & 12 &  4
\end{array}\right]}
\end{aligned}
$$

## Part (b)

$$
\begin{aligned}
  AB &= \left[\begin{array}{rrr}
    2 & -1 & 4\\
    0 & -1 & 0\\
    3 & -2 & 6
\end{array}\right]\left[\begin{array}{rrr}
    0 & 0 & 1\\
    1 & 0 & 0\\
    0 & 2 & 0
\end{array}\right]\\
       &= \left[\begin{array}{rrr}
    2 \times 0 - 1 \times 1 + 4 \times 0 & 2 \times 0 - 1 \times 0 + 4 \times 2 & 2 \times 1 - 1 \times 0 + 4 \times 0\\
    0 \times 0 - 1 \times 1 + 0 \times 0 & 0 \times 0 - 1 \times 0 + 0 \times 2 & 0 \times 1 - 1 \times 0 - 0 \times 0\\
    3 \times 0 - 2 \times 1 + 6 \times 0 & 3 \times 0 - 2 \times 0 + 6 \times 2 & 3 \times 1 - 2 \times 0 + 6 \times 0\\
\end{array}\right]\\
       &= \boxed{\left[\begin{array}{rrr}
    -1 &  8 &  2\\
    -1 &  0 &  0\\
    -2 & 12 &  3
\end{array}\right]}
\end{aligned}
$$

## Part (c)

$$
\begin{aligned}
  BA &= \left[\begin{array}{rrr}
    0 & 0 & 1\\
    1 & 0 & 0\\
    0 & 2 & 0
\end{array}\right]\left[\begin{array}{rrr}
    2 & -1 & 4\\
    0 & -1 & 0\\
    3 & -2 & 6
\end{array}\right]\\
       &= \left[\begin{array}{rrr}
    0 \times 2 + 0 \times 0 + 1 \times 3 & 0 \times -1 - 0 \times 1 - 1 \times 2 & 0 \times 4 + 0 \times 0 + 1 \times 6\\
    1 \times 2 + 0 \times 0 + 0 \times 3 & 1 \times -1 - 0 \times 1 - 0 \times 2 & 1 \times 4 + 0 \times 0 + 0 \times 6\\
    0 \times 2 + 2 \times 0 + 0 \times 3 & 0 \times -1 - 2 \times 1 - 0 \times 2 & 0 \times 4 + 2 \times 0 + 0 \times 6\\
\end{array}\right]\\
       &= \boxed{\left[\begin{array}{rrr}
     3 & -2 &  6\\
     2 & -1 &  4\\
     0 & -2 &  0
\end{array}\right]}
\end{aligned}
$$

## Part (d)

$$
\begin{aligned}
  B + A &= \left[\begin{array}{rrr}
    0 & 0 & 1\\
    1 & 0 & 0\\
    0 & 2 & 0
\end{array}\right] + \left[\begin{array}{rrr}
    2 & -1 & 4\\
    0 & -1 & 0\\
    3 & -2 & 6
\end{array}\right]\\
        &= \left[\begin{array}{rrr}
     0 + 2 & 0 - 1 & 1 + 4\\
     1 + 0 & 0 - 1 & 0 + 0\\
     0 + 3 & 2 - 2 & 0 + 6
\end{array}\right]\\
        &= \boxed{\left[\begin{array}{rrr}
     2 & -1 &  5\\
     1 & -1 &  0\\
     3 &  0 &  6
\end{array}\right]}
\end{aligned}
$$

## Part (e)

$$
\begin{aligned}
  B^T &= \left[\begin{array}{rrr}
    0 & 0 & 1\\
    1 & 0 & 0\\
    0 & 2 & 0
\end{array}\right]^T\\
        &= \boxed{\left[\begin{array}{rrr}
     0 & 1 & 0\\
     0 & 0 & 2\\
     1 & 0 & 0
\end{array}\right]}
\end{aligned}
$$

## Part (f)

Matrix $A$ is *not* invertable because its columns are not linearly independent (one column can be written as a linear-combination of the others):
$$
\left[\begin{array}{r}
     2\\
     0\\
     3
\end{array}\right] = 0 \times \left[\begin{array}{r}
     -1\\
     -1\\
     -2
\end{array}\right] + \frac{1}{2} \times \left[\begin{array}{r}
     4\\
     0\\
     6
\end{array}\right]
$$

On the other hand, matrix $B$ *is* invertable. To find its inverse, let us augment it with the identity matrix and perform row-reductions:
$$
\left[\begin{array}{rrr:rrr}
    0 & 0 & 1 & 1 & 0 & 0\\
    1 & 0 & 0 & 0 & 1 & 0\\
    0 & 2 & 0 & 0 & 0 & 1
\end{array}\right] \sim \left[\begin{array}{rrr:rrc}
    1 & 0 & 0 & 0 & 1 & 0\\
    0 & 1 & 0 & 0 & 0 & \frac{1}{2}\\
    0 & 0 & 1 & 1 & 0 & 0
\end{array}\right]
$$
Which tells us that the inverse of matrix $B$ is:
$$
B^{-1} = \left[\begin{array}{rrc}
    0 & 1 & 0\\
    0 & 0 & \frac{1}{2}\\
    1 & 0 & 0
\end{array}\right]
$$

