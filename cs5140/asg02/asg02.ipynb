{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "$\\text{}$\n",
    "\n",
    "$$\\LARGE\\text{Assignment: Document Hash}$$\n",
    "\n",
    "$$\\large\\text{Qianlang Chen (u1172983)}$$\n",
    "\n",
    "$$\\text{CS 5140 Spring 2021}$$\n",
    "\n",
    "$\\text{}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Problem 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = 4\n",
    "doc_path = 'data/D%d.txt'\n",
    "\n",
    "# Create the k-grams interested in this problem.\n",
    "G1, G2, G3 = [], [], []\n",
    "for doc in range(1, num_docs + 1):\n",
    "    contents = open(doc_path % doc, 'r').read().strip()\n",
    "    words = contents.split()\n",
    "    # G1: 2-grams based on words\n",
    "    G1.append({tuple(words[i:i+2]) for i in range(len(words) - 1)})\n",
    "    # G2: 3-grams based on words\n",
    "    G2.append({tuple(words[i:i+3]) for i in range(len(words) - 2)})\n",
    "    # G3: 3-grams based on characters\n",
    "    G3.append({contents[i:i+3] for i in range(len(contents) - 2)})"
   ]
  },
  {
   "source": [
    "## Part A"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\t\tG1\tG2\tG3\nDocument 1:\t161\t167\t482\nDocument 2:\t142\t147\t443\nDocument 3:\t390\t423\t977\nDocument 4:\t364\t381\t770\n"
     ]
    }
   ],
   "source": [
    "# Report the answers formatted nicely.\n",
    "print('\\t\\tG1\\tG2\\tG3')\n",
    "for i in range(num_docs):\n",
    "    print(f'Document {i + 1}:\\t{len(G1[i]):,}\\t{len(G2[i]):,}'\n",
    "          f'\\t{len(G3[i]):,}')"
   ]
  },
  {
   "source": [
    "## Part B"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Documents\tG1\tG2\tG3\n1 vs 2:\t\t0.8704\t0.8580\t0.9191\n1 vs 3:\t\t0.0110\t0.0017\t0.2449\n1 vs 4:\t\t0.0057\t0.0000\t0.2621\n2 vs 3:\t\t0.0095\t0.0018\t0.2348\n2 vs 4:\t\t0.0060\t0.0000\t0.2557\n3 vs 4:\t\t0.0121\t0.0012\t0.3135\n"
     ]
    }
   ],
   "source": [
    "# Computes and returns the Jaccard similarity between two sets `S` and\n",
    "# `T`.\n",
    "def jaccard(S, T): return len(S & T) / len(S | T)\n",
    "\n",
    "# Report the answers formatted nicely.\n",
    "print('Documents\\tG1\\tG2\\tG3')\n",
    "for i in range(num_docs - 1):\n",
    "    for j in range(i + 1, num_docs):\n",
    "        print(f'{i + 1} vs {j + 1}:\\t\\t{jaccard(G1[i], G1[j]):.4f}'\n",
    "              f'\\t{jaccard(G2[i], G2[j]):.4f}'\n",
    "              f'\\t{jaccard(G3[i], G3[j]):.4f}')"
   ]
  },
  {
   "source": [
    "$\\pagebreak$\n",
    "\n",
    "# Problem 2\n",
    "\n",
    "## Part A"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t\tJS_hat\tRun-time (s)\n",
      "100\t0.9300\t0.712\n",
      "200\t0.9300\t1.37\n",
      "400\t0.9275\t2.83\n",
      "800\t0.9275\t5.61\n",
      "1,600\t0.9250\t11.2\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# The size of the hash function `h`'s output space.\n",
    "m = 2**16\n",
    "\n",
    "# Hashes an object `x` using some random seed `i`. The random seed is\n",
    "# used to fake different hash functions using the XOR technique\n",
    "# described here: https://stackoverflow.com/a/19711615/157605 (thank\n",
    "# you, Bill Dimm). The output of this hash function will be within\n",
    "# [0, m).\n",
    "def h(x, i):\n",
    "    random.seed(i)\n",
    "    return hash(x) % m ^ random.randint(0, m - 1)\n",
    "\n",
    "# Estimates the Jaccard similarlity between two sets using the Fast\n",
    "# MinHash algorithm.\n",
    "def fast_min_hash(S, T, num_trials):\n",
    "    num_matches = 0\n",
    "    for i in range(num_trials):\n",
    "        min_hash_s = min(h(x, i) for x in S)\n",
    "        min_hash_t = min(h(x, i) for x in T)\n",
    "        if min_hash_s == min_hash_t: num_matches += 1\n",
    "    return num_matches / num_trials\n",
    "\n",
    "# Run the experiments and report the estimates of JS.\n",
    "T = [100, 200, 400, 800, 1600]\n",
    "print('t\\tJS_hat\\tRun-time (s)')\n",
    "for t in T:\n",
    "    start_time = time.time()\n",
    "    print(f'{t:,}\\t{fast_min_hash(G3[0], G3[1], t):.4f}'\n",
    "          f'\\t{(time.time() - start_time):.3g}')"
   ]
  },
  {
   "source": [
    "## Part B\n",
    "\n",
    "In this particular case, since the documents aren't so big, I'd go with $t = 1600$ since it gave an estimate closer to the true value (calculated in *Problem 1-B*, 0.9191) than the other $t$-values. However, I can see that this $t$ value wouldn't be practical for even documents just a bit bigger as the estimation would take too long."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}